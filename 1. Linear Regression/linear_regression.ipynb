{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_regression\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate data using scikit-learn\n",
    "n_samples = 1000\n",
    "n_features = 2\n",
    "X, y, true_coefficients = make_regression(n_samples=n_samples, n_features=n_features, coef=True, noise=1.0, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Ordinary Least Squares (OLS) Method:\n",
    "\n",
    "Derivation of the formula:\n",
    "\n",
    "- https://economictheoryblog.com/2015/02/19/ols_estimator/\n",
    "- https://stats.stackexchange.com/questions/46151/how-to-derive-the-least-square-estimator-for-multiple-linear-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alternatvie text](https://s0.wp.com/latex.php?latex=y%3D+Xb+%2B%5Cepsilon+&bg=ffffff&fg=2b2b2b&s=0&c=20201002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alternatvie text](https://miro.medium.com/v2/resize:fit:720/1*F6abOBjEz-X-ZwgDIw3fVw.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alternatvie text](https://cdn-images-1.medium.com/max/498/1*Bt1LjOBnr9vkwPTotwM2yA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation of model coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add intercept term to the data (Add a new column with ones for the intercept)\n",
    "X_b = np.c_[np.ones((n_samples, 1)), X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the transpose matrix of X\n",
    "X_b_transpose = X_b.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate coefficients by hand\n",
    "a = X_b_transpose.dot(X_b)\n",
    "a_inverse = np.linalg.inv(a)\n",
    "OLS_coefficients = a_inverse.dot(X_b_transpose).dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS Coefficients: [-0.04515316 41.06431679 40.09422036]\n"
     ]
    }
   ],
   "source": [
    "print(\"OLS Coefficients:\", OLS_coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Standard Errors:\n",
    "The standard errors measure the variability of the estimated coefficients and indicate the precision of the estimates. The formula for calculating the standard error of a coefficient is:\n",
    "\n",
    "$SE(β̂j) = √(MSE * Var(β̂j))$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $SE(β̂j)$ is the standard error of the coefficient β̂j.\n",
    "- $MSE$ is the Mean Squared Error, calculated as SSE / (n - p - 1), where SSE is the Sum of Squared Errors, n is the number of observations, and p is the number of predictors.\n",
    "- $Var(β̂j)$ is the variance of the coefficient β̂j, which is the (j, j) element of the inverse of the matrix $(X^T * X)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03015301, 0.03070767, 0.03093931])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate residuals\n",
    "residuals = y - X_b.dot(OLS_coefficients)\n",
    "\n",
    "# Calculate the degrees of freedom\n",
    "degrees_of_freedom = n_samples - n_features - 1\n",
    "\n",
    "# Calculate standard errors for coefficients\n",
    "MSE = np.sum(residuals ** 2) / degrees_of_freedom\n",
    "standard_errors = np.sqrt(np.diagonal(np.linalg.inv(X_b_transpose.dot(X_b)) * MSE))\n",
    "standard_errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard error (SE) of the coefficient estimate is a measure of the variability in the coefficient estimate. It quantifies the average amount of variation in the estimated coefficient. It considers the variability of the data, the complexity of the model (as reflected by the number of features), and the estimated variance of the error term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating T-Statistic:\n",
    "\n",
    "The t-statistic measures how many standard errors the estimated coefficient is away from zero. It's used to test whether the coefficient is significantly different from zero. The formula for calculating the t-statistic is:\n",
    "\n",
    "$t = β̂j / SE(β̂j)$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $t$ is the t-statistic for the coefficient $β̂j$.\n",
    "- $β̂j$ is the estimated coefficient.\n",
    "- $SE(β̂j)$ is the standard error of the coefficient $β̂j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  -1.49746738, 1337.26591798, 1295.89888656])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate t-values and p-values\n",
    "t_values = OLS_coefficients / standard_errors\n",
    "t_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to bear in mind that in the context of linear regression, when testing the significance of an individual coefficient, we are performing a **one-sample t-test**. This is because we are comparing the estimated coefficient (based on the sample data) to a hypothesized population parameter value (often the null hypothesis value, **which is typically zero for standard practice**). It makes sense, if the value of the coefficient on a population level equals zero, then this variable does not have an effect on the independent variable. \n",
    "\n",
    "A one-sample t-test compares the mean of a sample to a known value or hypothesized value, which is often referred to as the \"null value.\" In linear regression, the estimated coefficient represents the average change in the dependent variable associated with a one-unit change in the predictor variable. The one-sample t-test assesses whether this average change is significantly different from the hypothesized population parameter value (often zero for testing the null hypothesis of no effect).\n",
    "\n",
    "**NOTE**: In contrast, a two-sample t-test is used to compare the means of two separate groups (samples) to determine if they are significantly different from each other. This is not the same as the t-tests conducted in linear regression, which are focused on testing individual coefficients' significance.\n",
    "\n",
    "Reference: https://vasishth.github.io/Freq_CogSci/hypothesis-testing-the-one-sample-t-test.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating P-Value:\n",
    "\n",
    "The p-value measures the probability of observing a t-statistic as extreme as the one calculated, assuming that the null hypothesis is true (i.e., the coefficient is zero). A small p-value suggests that the coefficient is statistically significant. The formula for calculating the p-value is based on the t-distribution:\n",
    "\n",
    "$p = 2 * (1 - CDF_t(|t|, df))$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $p$ is the p-value associated with the coefficient.\n",
    "- $CDF_t$ is the cumulative distribution function of the t-distribution.\n",
    "- $|t|$ is the absolute value of the t-statistic.\n",
    "- $df$ is the degrees of freedom, which is $n - p - 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1345882, 0.       , 0.       ])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_values = 2 * (1 - stats.t.cdf(np.abs(t_values), df=degrees_of_freedom))\n",
    "p_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Independent Variables</th>\n",
       "      <th>True Coefficients</th>\n",
       "      <th>Estimated Coefficients</th>\n",
       "      <th>Standard Errors</th>\n",
       "      <th>t-Values</th>\n",
       "      <th>p-Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>const</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.045153</td>\n",
       "      <td>0.030153</td>\n",
       "      <td>-1.497467</td>\n",
       "      <td>0.134588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>x1</td>\n",
       "      <td>41.091573</td>\n",
       "      <td>41.064317</td>\n",
       "      <td>0.030708</td>\n",
       "      <td>1337.265918</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>x2</td>\n",
       "      <td>40.051046</td>\n",
       "      <td>40.094220</td>\n",
       "      <td>0.030939</td>\n",
       "      <td>1295.898887</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Independent Variables  True Coefficients  Estimated Coefficients  \\\n",
       "0                 const           0.000000               -0.045153   \n",
       "1                    x1          41.091573               41.064317   \n",
       "2                    x2          40.051046               40.094220   \n",
       "\n",
       "   Standard Errors     t-Values  p-Values  \n",
       "0         0.030153    -1.497467  0.134588  \n",
       "1         0.030708  1337.265918  0.000000  \n",
       "2         0.030939  1295.898887  0.000000  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_coefficients = np.insert(true_coefficients, 0, 0)\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'Independent Variables':['const', 'x1', 'x2'],\n",
    "    'True Coefficients': true_coefficients,\n",
    "    'Estimated Coefficients': OLS_coefficients,\n",
    "    'Standard Errors': standard_errors,\n",
    "    't-Values': t_values,\n",
    "    'p-Values': p_values\n",
    "})\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_summary(coefficients:np.array) -> pd.DataFrame:\n",
    "    \n",
    "    # Calculate residuals\n",
    "    residuals = y - X_b.dot(coefficients)\n",
    "\n",
    "    # Calculate the degrees of freedom\n",
    "    degrees_of_freedom = n_samples - n_features - 1\n",
    "\n",
    "    # Calculate standard errors for coefficients\n",
    "    MSE = np.sum(residuals ** 2) / degrees_of_freedom\n",
    "    standard_errors = np.sqrt(np.diagonal(np.linalg.inv(X_b_transpose.dot(X_b)) * MSE))\n",
    "\n",
    "    # Calculate t-values and p-values\n",
    "    t_values = coefficients / standard_errors\n",
    "\n",
    "    p_values = 2 * (1 - stats.t.cdf(np.abs(t_values), df=degrees_of_freedom))\n",
    "\n",
    "    # Summarise results\n",
    "    results_df = pd.DataFrame({\n",
    "        'Independent Variables':['const', 'x1', 'x2'],\n",
    "        'True Coefficients': true_coefficients,\n",
    "        'Estimated Coefficients': OLS_coefficients,\n",
    "        'Standard Errors': standard_errors,\n",
    "        't-Values': t_values,\n",
    "        'p-Values': p_values\n",
    "    })\n",
    "\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLS example with statsmodels API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(y,X_b)\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   1.000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   1.000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>1.710e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 19 Aug 2023</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:56:26</td>     <th>  Log-Likelihood:    </th> <td> -1369.5</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1000</td>      <th>  AIC:               </th> <td>   2745.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   997</td>      <th>  BIC:               </th> <td>   2760.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>   -0.0452</td> <td>    0.030</td> <td>   -1.497</td> <td> 0.135</td> <td>   -0.104</td> <td>    0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>   41.0643</td> <td>    0.031</td> <td> 1337.266</td> <td> 0.000</td> <td>   41.004</td> <td>   41.125</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>   40.0942</td> <td>    0.031</td> <td> 1295.899</td> <td> 0.000</td> <td>   40.034</td> <td>   40.155</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 3.817</td> <th>  Durbin-Watson:     </th> <td>   2.053</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.148</td> <th>  Jarque-Bera (JB):  </th> <td>   3.506</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.088</td> <th>  Prob(JB):          </th> <td>   0.173</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.769</td> <th>  Cond. No.          </th> <td>    1.04</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       1.000\n",
       "Model:                            OLS   Adj. R-squared:                  1.000\n",
       "Method:                 Least Squares   F-statistic:                 1.710e+06\n",
       "Date:                Sat, 19 Aug 2023   Prob (F-statistic):               0.00\n",
       "Time:                        21:56:26   Log-Likelihood:                -1369.5\n",
       "No. Observations:                1000   AIC:                             2745.\n",
       "Df Residuals:                     997   BIC:                             2760.\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         -0.0452      0.030     -1.497      0.135      -0.104       0.014\n",
       "x1            41.0643      0.031   1337.266      0.000      41.004      41.125\n",
       "x2            40.0942      0.031   1295.899      0.000      40.034      40.155\n",
       "==============================================================================\n",
       "Omnibus:                        3.817   Durbin-Watson:                   2.053\n",
       "Prob(Omnibus):                  0.148   Jarque-Bera (JB):                3.506\n",
       "Skew:                          -0.088   Prob(JB):                        0.173\n",
       "Kurtosis:                       2.769   Cond. No.                         1.04\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Gradient Descent Method:\n",
    "\n",
    "Calculating the Gradient of the **MSE** Cost Function\n",
    "\n",
    "Let's walk through the process of calculating the gradient of the cost function with respect to the coefficients $beta_0$ and $beta_1$ step by step:\n",
    "\n",
    "*1. Define the Linear Regression Model:*\n",
    "\n",
    "In linear regression, we have the model:\n",
    "$$ Y = \\beta_0 + \\beta_1 \\cdot X $$\n",
    "\n",
    "*2. Define the Cost Function:*\n",
    "\n",
    "The cost function measures the difference between our model's predictions and the actual target values. We use the Mean Squared Error (MSE) as the cost function:\n",
    "$$ \\text{MSE} = \\frac{1}{2m} \\sum_{i=1}^{m} (h_{\\beta}(X_i) - Y_i)^2 $$\n",
    "\n",
    "*3. Calculate the Gradient:*\n",
    "\n",
    "We want to find the gradient of the cost function with respect to the coefficients $ beta_0 $ and $ beta_1 $. The gradient points in the direction of steepest increase of the function.\n",
    "\n",
    "*Step 1: Calculate the Derivative with Respect to $ beta_0 $ :*\n",
    "$$ \\frac{\\partial}{\\partial \\beta_0} \\text{MSE} = \\frac{1}{m} \\sum_{i=1}^{m} (h_{\\beta}(X_i) - Y_i) $$\n",
    "\n",
    "*Step 2: Calculate the Derivative with Respect to $ beta_1 $:*\n",
    "$$ \\frac{\\partial}{\\partial \\beta_1} \\text{MSE} = \\frac{1}{m} \\sum_{i=1}^{m} (h_{\\beta}(X_i) - Y_i) \\cdot X_i $$\n",
    "\n",
    "*4. Gradient Vector:*\n",
    "\n",
    "The gradient vector combines the derivatives with respect to both coefficients:\n",
    "$$ \\nabla \\text{MSE} = \\left[ \\frac{\\partial}{\\partial \\beta_0} \\text{MSE}, \\frac{\\partial}{\\partial \\beta_1} \\text{MSE} \\right] $$\n",
    "\n",
    "*5. Using Matrix Notation:*\n",
    "\n",
    "We can express the gradient vector in matrix notation using the data matrix \\( X \\) and the vector of errors \\( h_{\\beta}(X) - Y \\):\n",
    "$$ \\nabla \\text{MSE} = \\frac{1}{m} X^T \\cdot (X \\cdot \\beta - Y) $$\n",
    "\n",
    "This matrix notation allows us to efficiently calculate the gradient using matrix operations, which is important for optimization algorithms like gradient descent.\n",
    "\n",
    "In summary, by calculating the gradient of the cost function, we determine the direction and magnitude of the changes needed in the coefficients to minimize the cost and improve the fit of our linear regression model to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we get the matrix notation from step 5 above?\n",
    "\n",
    "Let's break down the transformation from the gradient vector representation to the matrix notation step by step:\n",
    "\n",
    "*1. Gradient Vector Representation:*\n",
    "The gradient vector for the Mean Squared Error (MSE) cost function with respect to the coefficients $ beta_0 $ and $ beta_1 $ is given by:\n",
    "\n",
    "$ \\nabla \\text{MSE} = \\left[ \\frac{\\partial}{\\partial \\beta_0} \\text{MSE}, \\frac{\\partial}{\\partial \\beta_1} \\text{MSE} \\right] $\n",
    "\n",
    "*2. Step 1: Expand the Derivatives:*\n",
    "Recall that:\n",
    "- $ \\frac{\\partial}{\\partial \\beta_0} \\text{MSE} = \\frac{1}{m} \\sum_{i=1}^{m} (h_{\\beta}(X_i) - Y_i) $\n",
    "- $ \\frac{\\partial}{\\partial \\beta_1} \\text{MSE} = \\frac{1}{m} \\sum_{i=1}^{m} (h_{\\beta}(X_i) - Y_i) \\cdot X_i $\n",
    "\n",
    "*3. Step 2: Combine the Derivatives into a Vector:*\n",
    "Now we'll combine these derivatives into a vector form:\n",
    "$$ \\nabla \\text{MSE} = \\begin{bmatrix} \\frac{1}{m} \\sum_{i=1}^{m} (h_{\\beta}(X_i) - Y_i) \\\\ \\frac{1}{m} \\sum_{i=1}^{m} (h_{\\beta}(X_i) - Y_i) \\cdot X_i \\end{bmatrix} $$\n",
    "\n",
    "*4. Step 3: Use Matrix Multiplication:*\n",
    "Let \\( X \\cdot \\beta \\) denote the vector of predictions \\( h_{\\beta}(X) \\) for all data points. The gradient vector becomes:\n",
    "$$ \\nabla \\text{MSE} = \\frac{1}{m} \\begin{bmatrix} \\sum_{i=1}^{m} (X_i \\cdot \\beta - Y_i) \\\\ \\sum_{i=1}^{m} (X_i \\cdot \\beta - Y_i) \\cdot X_i \\end{bmatrix} $$\n",
    "\n",
    "*5. Step 4: Further Simplification:*\n",
    "We can express \\( X \\cdot \\beta - Y \\) as the residuals or errors of our predictions:\n",
    "$$ X \\cdot \\beta - Y = \\begin{bmatrix} (h_{\\beta}(X_1) - Y_1) \\\\ (h_{\\beta}(X_2) - Y_2) \\\\ \\vdots \\\\ (h_{\\beta}(X_m) - Y_m) \\end{bmatrix} $$\n",
    "\n",
    "*6. Step 5: Use Transpose for Matrix Multiplication:*\n",
    "Now, use the transpose of the data matrix \\( X \\), denoted as \\( X^T \\), to perform the matrix multiplication:\n",
    "$$ \\nabla \\text{MSE} = \\frac{1}{m} X^T \\begin{bmatrix} \\sum_{i=1}^{m} \\text{Error}i \\\\ \\sum{i=1}^{m} \\text{Error}_i \\cdot X_i \\end{bmatrix} $$\n",
    "\n",
    "*7. Step 6: Combine Everything:*\n",
    "Simplify the right-hand side to represent the full gradient vector:\n",
    "$$ \\nabla \\text{MSE} = \\frac{1}{m} X^T \\cdot \\begin{bmatrix} \\text{Error}_1 \\\\ \\text{Error}_2 \\\\ \\vdots \\\\ \\text{Error}_m \\end{bmatrix} = \\frac{1}{m} X^T \\cdot (X \\cdot \\beta - Y) $$\n",
    "\n",
    "And that's how we arrive at the matrix notation for the gradient of the cost function with respect to the coefficients in the context of linear regression! It's a more compact way of representing the calculations, leveraging matrix operations for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "num_iterations = 10_000\n",
    "\n",
    "# Initialize coefficients\n",
    "coefficients_gradient_descent = np.zeros(X_b.shape[1])\n",
    "coefficients_gradient_descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent\n",
    "for iteration in range(num_iterations):\n",
    "    # Calculate predicted values\n",
    "    predictions = X_b.dot(coefficients_gradient_descent)\n",
    "    \n",
    "    # Calculate residuals\n",
    "    residuals = predictions - y\n",
    "    \n",
    "    # Calculate gradients\n",
    "    gradients = (X_b.T.dot(residuals)) / n_samples\n",
    "    \n",
    "    # Update coefficients\n",
    "    coefficients_gradient_descent -= learning_rate * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Independent Variables</th>\n",
       "      <th>True Coefficients</th>\n",
       "      <th>Estimated Coefficients</th>\n",
       "      <th>Standard Errors</th>\n",
       "      <th>t-Values</th>\n",
       "      <th>p-Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>const</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.045153</td>\n",
       "      <td>0.030153</td>\n",
       "      <td>-1.497467</td>\n",
       "      <td>0.134588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>x1</td>\n",
       "      <td>41.091573</td>\n",
       "      <td>41.064317</td>\n",
       "      <td>0.030708</td>\n",
       "      <td>1337.265918</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>x2</td>\n",
       "      <td>40.051046</td>\n",
       "      <td>40.094220</td>\n",
       "      <td>0.030939</td>\n",
       "      <td>1295.898887</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Independent Variables  True Coefficients  Estimated Coefficients  \\\n",
       "0                 const           0.000000               -0.045153   \n",
       "1                    x1          41.091573               41.064317   \n",
       "2                    x2          40.051046               40.094220   \n",
       "\n",
       "   Standard Errors     t-Values  p-Values  \n",
       "0         0.030153    -1.497467  0.134588  \n",
       "1         0.030708  1337.265918  0.000000  \n",
       "2         0.030939  1295.898887  0.000000  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_summary(coefficients_gradient_descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "random_walk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9ef548cec2d662c67179413bd2710f228e5462ca4e1dfaa0cd325c3b313c3d33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
